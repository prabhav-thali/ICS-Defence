!pip install -q openpyxl pandas transformers torch scikit-learn

import pandas as pd
from datetime import datetime
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import os
import joblib

# Load Excel
try:
    file_path = "/kaggle/input/dec2019-2/Dec2019.xlsx"  # Upload manually in Colab or replace path
    hist_df = pd.read_excel(file_path, skiprows=9)
    print("Excel file loaded successfully.")
except Exception as e:
    print(f"Error loading Excel file: {e}")
    raise

# Convert t_stamp to datetime
try:
    hist_df['DATETIME'] = pd.to_datetime(hist_df['t_stamp'], format="%d/%m/%Y %H:%M:%S")
except Exception as e:
    print(f"Error converting timestamps: {e}")
    raise

# Define Attack Time Intervals
def in_range(dt, start_str, end_str):
    try:
        start = datetime.strptime(f"2019-12-06 {start_str}", "%Y-%m-%d %H:%M")
        end = datetime.strptime(f"2019-12-06 {end_str}", "%Y-%m-%d %H:%M")
        return start <= dt <= end
    except Exception as e:
        print(f"Error in in_range: {e}")
        return False

def label_attack(row):
    ts = row['DATETIME']
    attack_ranges = [
        ("10:30", "10:35"), ("10:45", "10:50"), ("11:00", "11:05"), ("11:15", "11:20"),
        ("12:30", "12:33"), ("12:43", "12:46"), ("12:56", "12:59"),
        ("13:09", "13:12"), ("13:22", "13:25")
    ]
    for start, end in attack_ranges:
        if in_range(ts, start, end):
            return 1
    return 0

# Apply attack labeling
hist_df['ATT_FLAG'] = hist_df.apply(label_attack, axis=1)

# Save processed data to CSV
try:
    hist_df.to_csv("processed_historian.csv", index=False)
    print("Processed data saved to processed_historian.csv")
except Exception as e:
    print(f"Error saving CSV: {e}")
    raise

## Part 2: BERT-Based Anomaly Detection

# Load processed historian
try:
    df = pd.read_csv("processed_historian.csv", low_memory=False)
    print("CSV loaded successfully.")
    # Check class distribution
    class_counts = df['ATT_FLAG'].value_counts()
    print(f"Class distribution - Normal (0): {class_counts.get(0, 0)}, Attack (1): {class_counts.get(1, 0)}")
except Exception as e:
    print(f"Error loading CSV: {e}")
    raise

# Create text features
features = [col for col in df.columns if col not in ['t_stamp', 'DATETIME', 'ATT_FLAG']]
df['text'] = df[features].astype(str).agg("|".join, axis=1)
df['label'] = df['ATT_FLAG']

# Split data
try:
    train_texts, val_texts, train_labels, val_labels = train_test_split(
        df['text'], df['label'], stratify=df['label'], test_size=0.2, random_state=42
    )
except Exception as e:
    print(f"Error splitting data: {e}")
    raise

# Tokenize
try:
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=128)
    val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=128)
except Exception as e:
    print(f"Error tokenizing data: {e}")
    raise

# Dataset Wrapper
class ICSDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = ICSDataset(train_encodings, list(train_labels))
val_dataset = ICSDataset(val_encodings, list(val_labels))

# Load model
try:
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
except Exception as e:
    print(f"Error loading model: {e}")
    raise

# Compute class weights
try:
    class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=df['label'])
    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)
    print(f"Class weights: {class_weights}")
except Exception as e:
    print(f"Error computing class weights: {e}")
    raise

# Custom Trainer with weighted loss
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get('labels')
        outputs = model(**inputs)
        logits = outputs.get('logits')
        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=50,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",  # Save model at each epoch
    load_best_model_at_end=True,  # Load best model based on evaluation
    metric_for_best_model="eval_loss",  # Use evaluation loss to pick best model
    greater_is_better=False  # Lower eval_loss is better
)

# Initialize trainer
trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train and evaluate
try:
    trainer.train()
    eval_results = trainer.evaluate()
    print(f"Evaluation results: {eval_results}")
except Exception as e:
    print(f"Error during training/evaluation: {e}")
    raise

# Save the model and tokenizer
save_dir = './swat_bert_model'
try:
    os.makedirs(save_dir, exist_ok=True)
    model.save_pretrained(save_dir)
    tokenizer.save_pretrained(save_dir)
    print(f"Model and tokenizer saved to {save_dir}")
except Exception as e:
    print(f"Error saving model/tokenizer: {e}")
    raise

## Part 3: Inference on New Data
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def load_and_predict(file_path, model_dir='./swat_bert_model'):
    try:
        # Load model and tokenizer
        model = BertForSequenceClassification.from_pretrained(model_dir)
        tokenizer = BertTokenizer.from_pretrained(model_dir)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model.to(device)
        model.eval()

        # Load and preprocess new data
        df = pd.read_csv(file_path, low_memory=False)
        features = [col for col in df.columns if col not in ['t_stamp', 'DATETIME', 'ATT_FLAG']]
        df['text'] = df[features].astype(str).agg("|".join, axis=1)
        
        # Tokenize
        encodings = tokenizer(list(df['text']), truncation=True, padding=True, max_length=128)
        dataset = ICSDataset(encodings, [0] * len(df))  # Dummy labels for inference
        
        # DataLoader
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False)
        
        # Predict
        predictions = []
        with torch.no_grad():
            for batch in dataloader:
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                outputs = model(**inputs)
                logits = outputs.logits
                preds = torch.argmax(logits, dim=1).cpu().numpy()
                predictions.extend(preds)
        
        # If labels are available, evaluate
        if 'ATT_FLAG' in df.columns:
            labels = df['ATT_FLAG'].values
            accuracy = accuracy_score(labels, predictions)
            precision = precision_score(labels, predictions)
            recall = recall_score(labels, predictions)
            f1 = f1_score(labels, predictions)
            print(f"Inference Results - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, "
                  f"Recall: {recall:.4f}, F1-Score: {f1:.4f}")
        
        df['PREDICTED_ATT_FLAG'] = predictions
        return df
    except Exception as e:
        print(f"Error during inference: {e}")
        return None
df_pred = load_and_predict('processed_historian.csv')

